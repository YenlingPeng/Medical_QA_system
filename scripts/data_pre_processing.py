# -*- coding: utf-8 -*-
"""data_pre_processing(editing)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-0ZZCJNoPmKxP8OvBZIkPYhpwjbsZvgH
"""

import os
import glob
from collections import Counter
from bs4 import BeautifulSoup
from ckiptagger import data_utils, construct_dictionary, WS, POS, NER
from gensim.models import word2vec
import numpy as np
import pandas as pd
import plotly.express as px
from tqdm import tqdm, trange
from sklearn.decomposition import PCA
import joblib
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch import optim

# Install ckip package
# !pip install -U ckiptagger[tfgpu,gdown]
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
# ws = WS("./data", disable_cuda=False)

data_all = []
tokenized_sentences_all = []
embeddings_all = []
articles_all = []
df = []
df_all = df
tokenized_articles_all = []

def main():
  # Tokenization 7 website
  udn()
  ca2()
  chimei()
  coomonhealth()
  jah()
  nowhealth()
  kenkon()

  # Token all sentences with CKIP
  for i in range(len(data_all)):
    tokenized_sentences = ws([d[2] for d in (tqdm(data_all[i]))]) 
    tokenized_sentences_all.append(tokenized_sentences)

  # Word2vec
  word2vec_model()


def word2vec_model():
  # Use a model traind on Wikipedia
  model = word2vec.Word2Vec.load('wiki_word2vec2.model')
  department_list = ['血液腫瘤科', '胸腔內科', '心臟科', '肝膽腸胃科', '神經內科', '感染科', '腎臟內科', '新陳代謝科', '免疫風濕科',
                   '乳房特診', '神經外科', '泌尿科', '骨科', '皮膚科', '眼科', '耳鼻喉科', '婦產科', '復健科', '家庭醫學科',
                   '放射腫瘤科', '兒科', '中醫科', '精神科', '牙科', '營養科', '整形外科']

  # Use department name to find label number >> For LSTM training >> supervise training
  cat2id = {department_list[i]: i for i in range(len(department_list))}
  vectorized_data = []
  for _ in range(7):
    for i in trange(len(tokenized_sentences_all[_])):
        vectors = [model.wv.get_vector(w).reshape(1, -1) for w in tokenized_sentences_all[_][i] if w in model.wv.vocab]
        if len(vectors) < 1:
            continue
        vectors = np.concatenate(vectors, axis=0)
        # Remove 外科的 vector
        if data_all[_][i][1] == '外科':
          continue
        vectorized_data.append(
            (
                vectors,
                cat2id[data_all[_][i][1]],
                
            )
        )
   # Save the vectorized data
   np.save('LSTM_test_vectorized_data_0415', vectorized_data)
      

def udn():
  global df_all, articles_all 
  df = pd.read_csv("./csvs/udn_revised.csv")
  df_all = df
  files = glob.glob("archives/udn_archive/*.html")
  data=[]
  articleword = ''
  for i in trange(len(df)):    
      category, disease, url = df.iloc[i][['category', 'disease', 'url']]
      filename = os.path.join('./archives/udn_archive', f'{category}_{disease}.html')
     
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p") + soup.find_all("li")
      if category == '外科':
          continue
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 10:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              #把每個句子append到data[]，資料結構：(文章編號, 科別, 句子)
              data.append((i, category, ss)) 
              articleword = articleword + ss
              if i %50 == 0: 
                print(ss)
      # articles_all.append((i, category, articleword))
  data_all.append(data)


def ca2():
  global df_all, articles_all
  df = pd.read_csv("./csvs/new_ca2_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/new_ca2_archive/*.html")
  data=[]
  articleword = ''
  for i in trange(len(df)):
      category, disease, url = df.iloc[i]["category"], df.iloc[i]["disease"], df.iloc[i][ "url"]
      if category == '外科':
          continue
      filename = os.path.join('./archives/new_ca2_archive', f'{category}_{disease}.html')
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p") + soup.find_all("div")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 40:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              # Append each sentence to data[]
              # Data structure: (article number, department, sentence)
              data.append((i, category, ss)) 
              articleword = articleword + ss

      # articles_all.append((i, category, articleword))
  data_all.append(data)


def chimei():
  global df_all
  df = pd.read_csv("./csvs/chimei_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/chimei_archive/*.html")
  data = []
  articleword = ''
  for i in trange(len(df)):
      category, disease, url = df.iloc[i][['category', 'disease', 'url']]
      
      if category == '外科':
          continue
      html = open(f"./archives/chimei_archive/{category}_{disease.replace('/', ' ')}.html")
      soup = BeautifulSoup(html)
      sentences = soup.find_all("p") + soup.find_all("div") + soup.find_all("li")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 40:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              data.append((i, category, ss)) 
              articleword = articleword + ss
      # articles_all.append((i, category, articleword))
  data_all.append(data)


def coomonhealth():
  global df_all, articles_all
  df = pd.read_csv("./csvs/commonhealth_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/commonhealth_archive/*.html")
  data = []
  articleword = ''
  for i in trange(len(df)):
      category, disease, url = df.iloc[i][['category', 'disease', 'url']]
      if category == '外科':
          continue
      filename = os.path.join('./archives/commonhealth_archive', f'{category}_{disease}.html')
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p") + soup.find_all("li")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 10:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              data.append((i, category, ss)) 
              articleword = articleword + ss
      # articles_all.append((i, category, articleword))
  data_all.append(data)
  

def jah():
  global df_all, articles_all
  df = pd.read_csv("./csvs/jah_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/jah_archive/*.html")
  data = []
  articleword = ''
  for i in trange(len(df)):
      category, disease, url = df.iloc[i]["category"], df.iloc[i]["disease"], df.iloc[i]["url"]

      if category == '外科':
          continue
      filename = os.path.join('./archives/jah_archive', f'{category}_{disease}.html')
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p") + soup.find_all("div") + soup.find_all("li")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 10:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              data.append((i, category, ss)) 
              articleword = articleword + ss
      # articles_all.append((i, category, articleword))
  data_all.append(data)


def nowhealth():
  global df_all, articles_all
  df = pd.read_csv("./csvs/nowhealth_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/nowhealth_archive/*.html")
  data = []
  articleword = ''
  for i in trange(len(df)):
      category, disease = df.iloc[i][['category', 'disease']]
      if category == '外科':
          continue
      filename = os.path.join('./archives/nowhealth_archive', f'{category}_{disease}.html')
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 10:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              data.append((i, category, ss)) 
              articleword = articleword + ss
      # articles_all.append((i, category, articleword))
  data_all.append(data)


def kenkon():
  global df_all, articles_all
  df = pd.read_csv("./csvs/kenkon_revised.csv")
  df_all = df_all.append(df)
  files = glob.glob("archives/nowhealth_archive/*.html")
  data = []
  articleword = ''
  for i in trange(len(df)):
      category, disease = df.iloc[i][['category', 'disease']]
      if category == '外科':
          continue
      filename = os.path.join('./archives/nowhealth_archive', f'{category}_{disease}.html')
      soup = BeautifulSoup(open(filename))
      sentences = soup.find_all("p")
      for s in sentences:
          if s.string is None:
              continue
          if len(s.string) < 10:
              continue
          for ss in s.string.split('。'): 
            if len(ss) > 1:
              data.append((i, category, ss)) 
              articleword = articleword + ss
      # articles_all.append((i, category, articleword))
  data_all.append(data)


if __name__ == "__main__":
  main()

